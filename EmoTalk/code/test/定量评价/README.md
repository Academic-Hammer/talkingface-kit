提供的代码是修改或补充上上级目录下的Windows环境可运行代码，直接将代码黏贴到EmoTalk_Win根文件夹下即可运行测试，当然我们也进行了封装，可使用网盘链接下载。

后续说明节选自《实验报告》的《3.1定量评估结果》。

**模型的定量评估结果**

由于本模型的主要工作在于解开语音中的不同情绪，并生成与之匹配的 3D面部表情，在评价部分我们选取了指标LSE-C和LSE-D来对模型效果进行评价，在模型中引入了SyncNet网络来完成这项工作。

在 SyncNet 中，LSE-D（Landmark-based SyncNet Distance）和LSE-C（Landmark-based SyncNetCorrelation）是两种用于评估视频和音频同步质量的评分指标。

LSE-D是基于面部关键点或唇部关键点计算的视频和音频同步距离，衡量视频帧和音频之间同步的误差，LSE-D值较小意味着视频和音频同步较好，而较大的值表示音频和视频之间有较大的不同步。

LSE-C是一个基于同步视频和音频之间的相关性的指标，用于衡量了视频和音频之间的相关性。模型基于提取的面部或唇部关键点，测量音频和视频帧之间的相关系数来确定同步效果。LSE-C值越大代表同步效果越好。

本模型对于所给测试用例的评估结果如下：

------------------------------------------------------------------------
      File_name               LSE-D ↓                    LSE-C ↑
------------------ -------------------------- --------------------------
      Jae-in.mp4              10.35534                  1.4022617
    
       Lieu.mp4              12.385286                   3.380148
    
      Macron.mp4              9.076749                  2.8200998
    
       May.mp4                9.209233                   4.897032
    
      Obama.mp4               8.938196                   4.982486
    
      Obama1.mp4              9.465542                   4.778718
    
      Obama2.mp4             10.280487                   3.501028
------------------------------------------------------------------------

表 1 模型定性评估结果

结果显示本模型在生成3-D动画Talkingface的任务中实现音视频同步的有效性和可靠性。在 LSE-D指标上，模型得到的同步距离在10左右，显示了视频帧和音频之间的高度一致性。这一结果表明，模型能够精确地捕捉视频中的面部关键点，并与音频信息高度融合，从而实现了较高的口型同步精度。在LSE-C 指标上模型也表现出了优异的性能。LSE-C值的提高反映了音频与视频之间的高度相关性，意味着视频帧的动作与音频信号之间的匹配度非常高。
