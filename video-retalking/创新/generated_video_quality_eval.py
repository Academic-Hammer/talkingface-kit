import os
import json
import argparse
import warnings
from transformers import AutoProcessor
from vllm import LLM, SamplingParams
from qwen_vl_utils import process_vision_info

# Suppress warnings
warnings.filterwarnings('ignore', category=UserWarning, message='TypedStorage is deprecated')

def run_qwen2_vl(model_path):
    """Initialize the Qwen-2-VL model and processor."""
    llm = LLM(model=model_path)
    processor = AutoProcessor.from_pretrained(model_path)
    return llm, processor

def run_inference(args):
    """Run inference for a single video."""
    # Initialize the model and processor
    llm, processor = run_qwen2_vl(args.model_path)
    
    # Define the evaluation prompt
    question = """
    This is a video generated by a talking face generation model. The task is to evaluate the quality of video frames while maintaining high visual fidelity and realistic expressions. Please evaluate the video based on the following criteria:

    1. Lip Movement Naturalness: Are the lip movements smooth and natural, considering the context of the video? (Score: 1-10)
    2. Visual Quality: Is the video clear and natural without artifacts, blurriness, or distortions? (Score: 1-10)
    3. Temporal Consistency: Are the transitions between video frames smooth and coherent? (Score: 1-10)
    4. Realism: Does the video look authentic and believable? (Score: 1-10)
    5. Identity Preservation: Does the person in the generated video retain their original facial features and identity? (Score: 1-10)

    Please provide:
    - A score for each criterion (1-10).
    - A brief explanation for your scores, highlighting any strengths or weaknesses observed in the video.
    """

    # Prepare input for the model
    video_path = args.video_path
    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {
            "role": "user",
            "content": [
                {
                    "type": "video",
                    "video": video_path,
                    "min_pixels": 224 * 224,
                    "max_pixels": 1280 * 28 * 28,
                },
                {"type": "text", "text": question},
            ],
        },
    ]

    prompt = processor.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )

    image_inputs, video_inputs = process_vision_info(messages)

    multi_modal_data = {}
    if image_inputs is not None:
        multi_modal_data["image"] = image_inputs
    if video_inputs is not None:
        multi_modal_data["video"] = video_inputs

    inputs = [{"prompt": prompt, "multi_modal_data": multi_modal_data}]

    # Sampling parameters
    sampling_params = SamplingParams(
        temperature=0.1, top_p=0.001, repetition_penalty=1.05, max_tokens=512,stop_token_ids=[]
    )

    # Generate response
    outputs = llm.generate(inputs, sampling_params=sampling_params)

    # Print output
    for output in outputs:
        generated_text = output.outputs[0].text
        print("Generated Evaluation:\n", generated_text)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Evaluate a single video using Qwen-2-VL.")
    parser.add_argument("--model_path", default= "./models/Qwen2-VL-7B-Instruct", type=str, help="Path to the Qwen-2-VL model.")
    parser.add_argument("--video_path", default= "./outputs/1_1.mp4", type=str, help="Path to the video file.")
    args = parser.parse_args()

    run_inference(args)